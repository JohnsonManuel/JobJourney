{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_groq import ChatGroq  # Import Groq client for handling the chat completions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OpenMP conflict\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "class Joblander:\n",
    "    def __init__(self):\n",
    "        # Initialize the LLM using Groq\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "            model_name=\"llama-3.1-70b-versatile\"\n",
    "        )\n",
    "\n",
    "        # Initialize the HuggingFace embedding model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Initialize FAISS index for similarity search\n",
    "        self.dimension = 384  # Assuming the HuggingFace model has a 384-dimensional output\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def embed_text(self, text):\n",
    "        \"\"\"Embed text using HuggingFace transformers.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def read_knowledge_base(self, file_path):\n",
    "        \"\"\"Read and return lines from the knowledge base text file.\"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def initialize_knowledge_base(self, knowledge_base_file):\n",
    "        \"\"\"Initialize FAISS index with embeddings from the knowledge base.\"\"\"\n",
    "        knowledge_base = self.read_knowledge_base(knowledge_base_file)\n",
    "        \n",
    "        # Embed the knowledge base into a list of vectors\n",
    "        knowledge_base_embeddings = [self.embed_text(text) for text in knowledge_base]\n",
    "        \n",
    "        # Convert the list of embeddings into a 2D numpy array\n",
    "        knowledge_base_embeddings = np.vstack(knowledge_base_embeddings)\n",
    "        \n",
    "        # Add the embeddings to the FAISS index\n",
    "        self.index.add(knowledge_base_embeddings)\n",
    "        self.knowledge_base = knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def query_rag(self, input_text):\n",
    "        \"\"\"Perform a similarity search on the FAISS index to get relevant context.\"\"\"\n",
    "        input_embedding = self.embed_text(input_text)\n",
    "        D, I = self.index.search(np.array(input_embedding), k=5)\n",
    "        retrieved_texts = [self.knowledge_base[i] for i in I[0]]\n",
    "        context = \"\\n\".join(retrieved_texts)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def generate_response(self, messages, model=\"llama3-8b-8192\", temperature=0.5, max_tokens=1024):\n",
    "        \"\"\"Generate a response from the Groq LLM using streaming.\"\"\"\n",
    "        stream = self.llm(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            stop=None,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Collect the generated output\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Avoid concatenating None\n",
    "                result += content\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def optimize_resume(self, job_description, current_resume):\n",
    "        \"\"\"Optimize a resume based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume optimization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nOptimize this resume:\\n{current_resume}\\n\\nJob Description: {job_description}\\nOptimized Resume:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, clean_up_tokenization_spaces=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\johns\\anaconda3\\envs\\rag-env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:98: Error: 'f' failed: could not open /vector_stores\\new_vector_store.faiss for writing: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 154\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    153\u001b[0m     joblander \u001b[38;5;241m=\u001b[39m Joblander()\n\u001b[1;32m--> 154\u001b[0m     \u001b[43mjoblander\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_knowledge_base\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresult.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m     job_description \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSoftware Engineer position at XYZ Corp focusing on Python and machine learning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m     current_resume \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperienced software engineer with skills in Python, C++, and cloud computing.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[12], line 71\u001b[0m, in \u001b[0;36mJoblander.initialize_knowledge_base\u001b[1;34m(self, knowledge_base_file)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Save the new FAISS index to a file\u001b[39;00m\n\u001b[0;32m     70\u001b[0m store_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvector_store_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_vector_store.faiss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 71\u001b[0m \u001b[43mfaiss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew vector store saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstore_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\johns\\anaconda3\\envs\\rag-env\\lib\\site-packages\\faiss\\swigfaiss_avx2.py:10403\u001b[0m, in \u001b[0;36mwrite_index\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m  10402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_index\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m> 10403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_swigfaiss_avx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error in __cdecl faiss::FileIOWriter::FileIOWriter(const char *) at D:\\a\\faiss-wheels\\faiss-wheels\\faiss\\faiss\\impl\\io.cpp:98: Error: 'f' failed: could not open /vector_stores\\new_vector_store.faiss for writing: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_groq import ChatGroq  # Import Groq client for handling the chat completions\n",
    "import glob\n",
    "\n",
    "# Avoid OpenMP conflict\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "class Joblander:\n",
    "    def __init__(self, vector_store_path=\"/vector_stores\"):\n",
    "        # Initialize the LLM using Groq\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "            model_name=\"llama-3.1-70b-versatile\"\n",
    "        )\n",
    "\n",
    "        # Initialize the HuggingFace embedding model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Initialize FAISS index for similarity search\n",
    "        self.dimension = 384  # Assuming the HuggingFace model has a 384-dimensional output\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # Store the vector store path\n",
    "        self.vector_store_path = vector_store_path\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        \"\"\"Embed text using HuggingFace transformers.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        return embeddings\n",
    "\n",
    "    def read_knowledge_base(self, file_path):\n",
    "        \"\"\"Read and return lines from the knowledge base text file.\"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "        return data\n",
    "\n",
    "    def initialize_knowledge_base(self, knowledge_base_file):\n",
    "        \"\"\"Initialize FAISS index with embeddings from the knowledge base or load existing FAISS indexes.\"\"\"\n",
    "        # Check if vector stores exist in the specified directory\n",
    "        vector_store_files = glob.glob(os.path.join(self.vector_store_path, \"*.faiss\"))\n",
    "        \n",
    "        if vector_store_files:\n",
    "            # Load the existing vector store files into the FAISS index\n",
    "            for store_file in vector_store_files:\n",
    "                faiss_index = faiss.read_index(store_file)\n",
    "                self.index.merge_from(faiss_index)\n",
    "            print(f\"Loaded existing vector stores from {self.vector_store_path}\")\n",
    "        else:\n",
    "            # If no existing stores, initialize from the knowledge base\n",
    "            knowledge_base = self.read_knowledge_base(knowledge_base_file)\n",
    "            \n",
    "            # Embed the knowledge base into a list of vectors\n",
    "            knowledge_base_embeddings = [self.embed_text(text) for text in knowledge_base]\n",
    "            \n",
    "            # Convert the list of embeddings into a 2D numpy array\n",
    "            knowledge_base_embeddings = np.vstack(knowledge_base_embeddings)\n",
    "            \n",
    "            # Add the embeddings to the FAISS index\n",
    "            self.index.add(knowledge_base_embeddings)\n",
    "            self.knowledge_base = knowledge_base\n",
    "            \n",
    "            # Save the new FAISS index to a file\n",
    "            store_file_path = os.path.join(self.vector_store_path, \"new_vector_store.faiss\")\n",
    "            faiss.write_index(self.index, store_file_path)\n",
    "            print(f\"New vector store saved to {store_file_path}\")\n",
    "\n",
    "    def query_rag(self, input_text):\n",
    "        \"\"\"Perform a similarity search on the FAISS index to get relevant context.\"\"\"\n",
    "        input_embedding = self.embed_text(input_text)\n",
    "        D, I = self.index.search(np.array(input_embedding), k=5)\n",
    "        retrieved_texts = [self.knowledge_base[i] for i in I[0]]\n",
    "        context = \"\\n\".join(retrieved_texts)\n",
    "        return context\n",
    "\n",
    "    def generate_response(self, messages, model=\"llama3-8b-8192\", temperature=0.5, max_tokens=1024):\n",
    "        \"\"\"Generate a response from the Groq LLM using streaming.\"\"\"\n",
    "        stream = self.llm(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            stop=None,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Collect the generated output\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Avoid concatenating None\n",
    "                result += content\n",
    "\n",
    "        return result\n",
    "\n",
    "    def optimize_resume(self, job_description, current_resume):\n",
    "        \"\"\"Optimize a resume based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume optimization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nOptimize this resume:\\n{current_resume}\\n\\nJob Description: {job_description}\\nOptimized Resume:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def write_cover_letter(self, job_description):\n",
    "        \"\"\"Write a cover letter based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cover letter assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nWrite a cover letter for the following job description:\\n{job_description}\\n\\nCover Letter:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def write_cold_email(self, job_description):\n",
    "        \"\"\"Write a cold email based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cold email assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nWrite a professional cold email for the following job description:\\n{job_description}\\n\\nCold Email:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def chat_agent(self, query):\n",
    "        \"\"\"AI Assistant to guide the job seeker in career advice.\"\"\"\n",
    "        context = self.query_rag(query)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a career guidance assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nBased on the query '{query}', provide guidance to help the job seeker improve their chances of getting a job. Suggest courses, projects, and steps they should take.\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    joblander = Joblander()\n",
    "    joblander.initialize_knowledge_base(\"result.txt\")\n",
    "    \n",
    "    job_description = \"Software Engineer position at XYZ Corp focusing on Python and machine learning.\"\n",
    "    current_resume = \"Experienced software engineer with skills in Python, C++, and cloud computing.\"\n",
    "    \n",
    "    optimized_resume = joblander.optimize_resume(job_description, current_resume)\n",
    "    cover_letter = joblander.write_cover_letter(job_description)\n",
    "    cold_email = joblander.write_cold_email(job_description)\n",
    "    \n",
    "    print(\"Optimized Resume:\\n\", optimized_resume)\n",
    "    print(\"\\nCover Letter:\\n\", cover_letter)\n",
    "    print(\"\\nCold Email:\\n\", cold_email)\n",
    "    \n",
    "    # AI Assistant Query Example\n",
    "    query = \"I want to become a machine learning engineer, what should I do?\"\n",
    "    assistant_response = joblander.chat_agent(query)\n",
    "    print(\"\\nAI Assistant Response:\\n\", assistant_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_groq import ChatGroq  # Import Groq client for handling the chat completions\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid OpenMP conflict\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain_groq import ChatGroq  # Import Groq client for handling the chat completions\n",
    "import glob\n",
    "\n",
    "# Avoid OpenMP conflict\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "class Joblander:\n",
    "    def __init__(self, vector_store_path=\"/user_1_git_faiss_store\"):\n",
    "        # Initialize the LLM using Groq\n",
    "        self.llm = ChatGroq(\n",
    "            temperature=0,\n",
    "            groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "            model_name=\"llama-3.1-70b-versatile\"\n",
    "        )\n",
    "\n",
    "        # Initialize the HuggingFace embedding model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        self.model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "        # Initialize FAISS index for similarity search\n",
    "        self.dimension = 384  # Assuming the HuggingFace model has a 384-dimensional output\n",
    "        self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        \n",
    "        # Store the vector store path\n",
    "        self.vector_store_path = vector_store_path\n",
    "\n",
    "    def embed_text(self, text):\n",
    "        \"\"\"Embed text using HuggingFace transformers.\"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, clean_up_tokenization_spaces=False)\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "        return embeddings\n",
    "\n",
    "    def read_knowledge_base(self, file_path):\n",
    "        \"\"\"Read and return lines from the knowledge base text file.\"\"\"\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.readlines()\n",
    "        return data\n",
    "\n",
    "    def initialize_knowledge_base(self, knowledge_base_file):\n",
    "        \"\"\"Initialize FAISS index with embeddings from the knowledge base or load existing FAISS indexes.\"\"\"\n",
    "        # Check if vector stores exist in the specified directory\n",
    "        vector_store_files = glob.glob(os.path.join(self.vector_store_path, \"*.faiss\"))\n",
    "        \n",
    "        if vector_store_files:\n",
    "            # Load the existing vector store files into the FAISS index\n",
    "            for store_file in vector_store_files:\n",
    "                faiss_index = faiss.read_index(store_file)\n",
    "                self.index.merge_from(faiss_index)\n",
    "            print(f\"Loaded existing vector stores from {self.vector_store_path}\")\n",
    "        else:\n",
    "            # If no existing stores, initialize from the knowledge base\n",
    "            knowledge_base = self.read_knowledge_base(knowledge_base_file)\n",
    "            \n",
    "            # Embed the knowledge base into a list of vectors\n",
    "            knowledge_base_embeddings = [self.embed_text(text) for text in knowledge_base]\n",
    "            \n",
    "            # Convert the list of embeddings into a 2D numpy array\n",
    "            knowledge_base_embeddings = np.vstack(knowledge_base_embeddings)\n",
    "            \n",
    "            # Add the embeddings to the FAISS index\n",
    "            self.index.add(knowledge_base_embeddings)\n",
    "            self.knowledge_base = knowledge_base\n",
    "            \n",
    "            # Save the new FAISS index to a file\n",
    "            store_file_path = os.path.join(self.vector_store_path, \"new_vector_store.faiss\")\n",
    "            faiss.write_index(self.index, store_file_path)\n",
    "            print(f\"New vector store saved to {store_file_path}\")\n",
    "\n",
    "    def query_rag(self, input_text):\n",
    "        \"\"\"Perform a similarity search on the FAISS index to get relevant context.\"\"\"\n",
    "        input_embedding = self.embed_text(input_text)\n",
    "        D, I = self.index.search(np.array(input_embedding), k=5)\n",
    "        retrieved_texts = [self.knowledge_base[i] for i in I[0]]\n",
    "        context = \"\\n\".join(retrieved_texts)\n",
    "        return context\n",
    "\n",
    "    def generate_response(self, messages, model=\"llama3-8b-8192\", temperature=0.5, max_tokens=1024):\n",
    "        \"\"\"Generate a response from the Groq LLM using streaming.\"\"\"\n",
    "        stream = self.llm(\n",
    "            messages=messages,\n",
    "            model=model,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1,\n",
    "            stop=None,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        # Collect the generated output\n",
    "        result = \"\"\n",
    "        for chunk in stream:\n",
    "            content = chunk.choices[0].delta.content\n",
    "            if content:  # Avoid concatenating None\n",
    "                result += content\n",
    "\n",
    "        return result\n",
    "\n",
    "    def optimize_resume(self, job_description, current_resume):\n",
    "        \"\"\"Optimize a resume based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a resume optimization assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nOptimize this resume:\\n{current_resume}\\n\\nJob Description: {job_description}\\nOptimized Resume:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def write_cover_letter(self, job_description):\n",
    "        \"\"\"Write a cover letter based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cover letter assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nWrite a cover letter for the following job description:\\n{job_description}\\n\\nCover Letter:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def write_cold_email(self, job_description):\n",
    "        \"\"\"Write a cold email based on the job description.\"\"\"\n",
    "        context = self.query_rag(job_description)\n",
    "        \n",
    "        # Properly structured messages for Groq\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a cold email assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nWrite a professional cold email for the following job description:\\n{job_description}\\n\\nCold Email:\"}\n",
    "        ]\n",
    "        \n",
    "        return self.generate_response(messages)\n",
    "\n",
    "    def chat_agent(self, query):\n",
    "        \"\"\"AI Assistant to guide the job seeker in career advice.\"\"\"\n",
    "        context = self.query_rag(query)\n",
    "        \n",
    "        # Properly structured messages for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
